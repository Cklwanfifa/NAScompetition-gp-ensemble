{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b76edcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cplfw_rank': 9, 'market1501_rank': 361, 'dukemtmc_rank': 426, 'msmt17_rank': 433, 'veri_rank': 327, 'vehicleid_rank': 480, 'veriwild_rank': 425, 'sop_rank': 367, 'arch': 'l231131331121121331111211121331321321'}\n",
      "train_num: 500\n",
      "[0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0] 100\n"
     ]
    }
   ],
   "source": [
    "#有一些严重的Bug被发现：需要做一个bug-fix版本的提交\n",
    "import json\n",
    "#from Preprocessing import Preprocessing, Label_Transformer\n",
    "import scipy\n",
    "import scipy.stats\n",
    "\n",
    "with open('CVPR_2022_NAS_Track2_train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "print(train_data['arch1'])\n",
    "print('train_num:',len(train_data.keys()))\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Preprocessing_1:\n",
    "    def __init__(self):\n",
    "        self.arch_record = []\n",
    "\n",
    "    def convert_x(self, arch_str):\n",
    "        temp_arch = []\n",
    "        total_1 = 0\n",
    "        total_2 = 0\n",
    "        ts = ''\n",
    "        for i in range(len(arch_str)):\n",
    "            if i % 3 != 0 and i != 0 and i <= 30:\n",
    "                elm = arch_str[i]\n",
    "                ts = ts + elm\n",
    "                if elm == 'l' or elm == '1':\n",
    "                    temp_arch = temp_arch + [1, 1, 0, 0]\n",
    "                elif elm == 'j' or elm == '2':\n",
    "                    temp_arch = temp_arch + [0, 1, 1, 0]\n",
    "                elif elm == 'k' or elm == '3':\n",
    "                    temp_arch = temp_arch + [0, 0, 1, 1]\n",
    "                else:\n",
    "                    temp_arch = temp_arch + [0, 0, 0, 0]\n",
    "            \n",
    "            elif i % 3 != 0 and i != 0 and i > 30:\n",
    "                elm = arch_str[i]\n",
    "                if elm == 'l' or elm == '1':\n",
    "                    temp_arch = temp_arch + [1, 1, 0, 0, 0]\n",
    "                elif elm == 'j' or elm == '2':\n",
    "                    temp_arch = temp_arch + [0, 1, 1, 0, 0]\n",
    "                elif elm == 'k' or elm == '3':\n",
    "                    temp_arch = temp_arch + [0, 0, 1, 1, 0]\n",
    "                else:\n",
    "                    temp_arch = temp_arch + [0, 0, 0, 0, 1]\n",
    "            \n",
    "\n",
    "        return temp_arch\n",
    "    \n",
    "    def process_x(self, train_data):\n",
    "        train_list = [[], [], [], [], [], [], [], []]\n",
    "        arch_list_train = []\n",
    "        name_list = ['cplfw_rank', 'market1501_rank', 'dukemtmc_rank', 'msmt17_rank', 'veri_rank', 'vehicleid_rank', 'veriwild_rank', 'sop_rank']\n",
    "        for key in train_data.keys():\n",
    "            for idx, name in enumerate(name_list):\n",
    "                train_list[idx].append(train_data[key][name])\n",
    "            arch_list_train.append(self.convert_x(train_data[key]['arch']))\n",
    "\n",
    "        return train_list, arch_list_train\n",
    "\n",
    "processor_1 = Preprocessing_1()\n",
    "train_list, arch_list_train_1 = processor_1.process_x(train_data)\n",
    "n_feature = len(arch_list_train_1[0])\n",
    "print(arch_list_train_1[0], n_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b54d3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cplfw_rank': 9, 'market1501_rank': 361, 'dukemtmc_rank': 426, 'msmt17_rank': 433, 'veri_rank': 327, 'vehicleid_rank': 480, 'veriwild_rank': 425, 'sop_rank': 367, 'arch': 'l231131331121121331111211121331321321'}\n",
      "train_num: 500\n",
      "[0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0] 100\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "#from Preprocessing import Preprocessing, Label_Transformer\n",
    "import scipy\n",
    "import scipy.stats\n",
    "\n",
    "with open('CVPR_2022_NAS_Track2_train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "print(train_data['arch1'])\n",
    "print('train_num:',len(train_data.keys()))\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Preprocessing_2:\n",
    "    def __init__(self):\n",
    "        self.arch_record = []\n",
    "\n",
    "    def convert_x(self, arch_str):\n",
    "        temp_arch = []\n",
    "        total_1 = 0\n",
    "        total_2 = 0\n",
    "        ts = ''\n",
    "        for i in range(len(arch_str)):\n",
    "            '''\n",
    "            if i == 0:\n",
    "                elm = arch_str[i]\n",
    "                if elm == 'l' or elm == '1':\n",
    "                    temp_arch = temp_arch + [1, 0, 0, 0]\n",
    "                elif elm == 'j' or elm == '2':\n",
    "                    temp_arch = temp_arch + [0, 1, 0, 0]\n",
    "                elif elm == 'k' or elm == '3':\n",
    "                    temp_arch = temp_arch + [0, 0, 1, 0]\n",
    "                else:\n",
    "                    temp_arch = temp_arch + [0, 0, 0, 1]\n",
    "            '''\n",
    "            if i % 3 != 0 and i != 0 and i <= 30:\n",
    "                elm = arch_str[i]\n",
    "                ts = ts + elm\n",
    "                if elm == 'l' or elm == '1':\n",
    "                    temp_arch = temp_arch + [1, 0, 0, 0]\n",
    "                elif elm == 'j' or elm == '2':\n",
    "                    temp_arch = temp_arch + [0, 1, 0, 0]\n",
    "                elif elm == 'k' or elm == '3':\n",
    "                    temp_arch = temp_arch + [0, 0, 1, 0]\n",
    "                else:\n",
    "                    temp_arch = temp_arch + [0, 0, 0, 1]\n",
    "            \n",
    "            elif i % 3 != 0 and i != 0 and i > 30:\n",
    "                elm = arch_str[i]\n",
    "                if elm == 'l' or elm == '1':\n",
    "                    temp_arch = temp_arch + [1, 0, 0, 0, 0]\n",
    "                elif elm == 'j' or elm == '2':\n",
    "                    temp_arch = temp_arch + [0, 1, 0, 0, 0]\n",
    "                elif elm == 'k' or elm == '3':\n",
    "                    temp_arch = temp_arch + [0, 0, 1, 0, 0]\n",
    "                else:\n",
    "                    temp_arch = temp_arch + [0, 0, 0, 1, 0]\n",
    "\n",
    "        return temp_arch\n",
    "    \n",
    "    def process_x(self, train_data):\n",
    "        train_list = [[], [], [], [], [], [], [], []]\n",
    "        arch_list_train = []\n",
    "        name_list = ['cplfw_rank', 'market1501_rank', 'dukemtmc_rank', 'msmt17_rank', 'veri_rank', 'vehicleid_rank', 'veriwild_rank', 'sop_rank']\n",
    "        for key in train_data.keys():\n",
    "            for idx, name in enumerate(name_list):\n",
    "                train_list[idx].append(train_data[key][name])\n",
    "            arch_list_train.append(self.convert_x(train_data[key]['arch']))\n",
    "\n",
    "        return train_list, arch_list_train\n",
    "\n",
    "processor_2 = Preprocessing_2()\n",
    "train_list, arch_list_train_2 = processor_2.process_x(train_data)\n",
    "n_feature = len(arch_list_train_2[0])\n",
    "print(arch_list_train_2[0], n_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88b6256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2021  PaddlePaddle Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\"\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "import numba as nb\n",
    "import itertools\n",
    "\n",
    "\n",
    "__all__ = [\"GPNAS\"]\n",
    "\n",
    "def get_weight(**kwargs):\n",
    "    res = []\n",
    "    for i in range(24):\n",
    "        key = 'w' + str(i)\n",
    "        if i < 20:\n",
    "            sub_weight = [kwargs.get(key)] * 4\n",
    "        else:\n",
    "            sub_weight = [kwargs.get(key)] * 5\n",
    "        res = res + sub_weight\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "def get_correlation(mat1, mat2, task, weight):\n",
    "    \n",
    "    mat1 = mat1[:n_feature]\n",
    "    mat2 = mat2[:n_feature]\n",
    "\n",
    "    mat_diff = np.abs(np.array(mat1) - np.array(mat2))\n",
    "    if np.sum(mat_diff) == 0:\n",
    "        return 1\n",
    "    \n",
    "\n",
    "    def get_f_index(x):\n",
    "        return [x*4, 4*x+1, 4*x+2]\n",
    "\n",
    "    \n",
    "    w1 = weight\n",
    "    diffs = np.dot(mat_diff * np.array(w1), mat_diff)\n",
    "    k1 = 1 * np.exp(-np.sqrt(diffs) / 28)\n",
    "    \n",
    "    diffs_2 = np.dot(mat_diff, mat_diff)\n",
    "    k2 = 1 * np.exp(-np.sqrt(diffs_2) / 28)\n",
    "    \n",
    "    #return k1\n",
    "    return k1 * 0.62 + k2 * 0.38\n",
    "\n",
    "\n",
    "def get_cor_mat_joint(X, X_train, task, weight):\n",
    "    X = np.array(X)\n",
    "    X_train = np.array(X_train)\n",
    "    l_c = X.shape[0]\n",
    "    l_r = X_train.shape[0]\n",
    "    \n",
    "    cor_mat = np.array([get_correlation(c_mat, r_mat, task, weight) for c_mat in X for r_mat in X_train]).reshape((l_c, l_r))\n",
    "\n",
    "    return np.mat(cor_mat)\n",
    "\n",
    "\n",
    "\n",
    "class GPNAS(object):\n",
    "    \"\"\"\n",
    "    GPNAS(Gaussian Process based Neural Architecture Search) is a neural architecture search algorithm.\n",
    "    We model the correlation between architectue and performance from a Bayesian perspective. Specifically, by introducing a novel Gaussian Process based\n",
    "    NAS (GP-NAS) method, the correlations are modeled by the kernel function and mean function. The kernel function is also learnable to enable adaptive modeling for complex\n",
    "    correlations in different search spaces. Furthermore, by in-corporating a mutual information based sampling method, we can theoretically ensure the high-performance\n",
    "    architecture with only a small set of samples. After addressing these problems, training GP-NAS once enables direct performance prediction of any architecture in different\n",
    "    scenarios and may obtain efficient networks for different deployment platforms.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, c_flag, m_flag):\n",
    "        self.hp_mat = 0.001\n",
    "        self.hp_cov = 0.01\n",
    "        self.cov_w = None\n",
    "        self.w = None\n",
    "        self.c_flag = c_flag\n",
    "        self.m_flag = m_flag\n",
    "    \n",
    "    def set_task(self, task):\n",
    "        self.task = task\n",
    "        \n",
    "    def set_weight(self, weight):\n",
    "        self.weight = weight\n",
    "\n",
    "    \n",
    "    def _get_corelation(self, mat1, mat2):\n",
    "        return get_correlation(mat1, mat2, self.task, self.weight)\n",
    "\n",
    "    def _preprocess_X(self, X):\n",
    "        \"\"\"\n",
    "        preprocess of input feature/ tokens of architecture\n",
    "        more complicated preprocess can be added such as nonlineaer transformation\n",
    "        \"\"\"\n",
    "\n",
    "        X = X.tolist()\n",
    "        p_X = copy.deepcopy(X)\n",
    "\n",
    "        for feature in p_X:\n",
    "            feature.append(1)\n",
    "\n",
    "        return p_X\n",
    "\n",
    "    def _get_cor_mat(self, X):\n",
    "        X = np.array(X)\n",
    "        l = X.shape[0]\n",
    "        cor_mat = []\n",
    "\n",
    "        for c_idx in range(l):\n",
    "            col = []\n",
    "            c_mat = X[c_idx].copy()\n",
    "\n",
    "            for r_idx in range(l):\n",
    "                r_mat = X[r_idx].copy()\n",
    "                temp_cor = self._get_corelation(c_mat, r_mat)\n",
    "                col.append(temp_cor)\n",
    "            cor_mat.append(col)\n",
    "\n",
    "        return np.mat(cor_mat)\n",
    "\n",
    "    def _get_cor_mat_joint(self, X, X_train):\n",
    "        res = get_cor_mat_joint(X, X_train, self.task, self.weight)\n",
    "\n",
    "        return np.mat(res)\n",
    "\n",
    "    def get_predict(self, X):\n",
    "        \"\"\"\n",
    "        get the prediction of network architecture X\n",
    "        \"\"\"\n",
    "        X = self._preprocess_X(X)\n",
    "        X = np.mat(X)\n",
    "        #print(X.shape)\n",
    "        #print(X[0])\n",
    "\n",
    "        #y1 =  X * self.w.reshape(-1,1)\n",
    "        #y2 =  neigh.predict(X[:,:96]).reshape(-1,1)\n",
    "        #return 0.7 * y1 + 0.3 * y2\n",
    "        return X * self.w\n",
    "\n",
    "    def get_predict_jiont(self, X, X_train, Y_train):\n",
    "        \"\"\"\n",
    "        get the prediction of network architecture X based on X_train and Y_train\n",
    "        \"\"\"\n",
    "        #X = self._preprocess_X(X)\n",
    "        X = np.mat(X)\n",
    "        X_train = np.mat(X_train)\n",
    "        Y_train = np.mat(Y_train)\n",
    "        m_X = self.get_predict(X)\n",
    "        m_X_train = self.get_predict(X_train)\n",
    "        mat_train = self._get_cor_mat(X_train)\n",
    "        mat_joint = self._get_cor_mat_joint(X, X_train)\n",
    "\n",
    "        return m_X + mat_joint * np.linalg.inv(mat_train + self.hp_mat * np.eye(\n",
    "            X_train.shape[0])) * (Y_train.T - m_X_train)\n",
    "\n",
    "    def get_initial_mean(self, X, Y):\n",
    "        \"\"\"\n",
    "        get initial mean of w\n",
    "        \"\"\"\n",
    "\n",
    "        X = self._preprocess_X(X)\n",
    "        X = np.mat(X)\n",
    "        Y = np.mat(Y)\n",
    "        A = np.linalg.inv(X.T * X + self.hp_mat * np.eye(X.shape[\n",
    "            1]))\n",
    "        B = X.T\n",
    "        C = Y.T\n",
    "        print(f'A,B,C shape is {A.shape}, {B.shape}, {C.shape}')\n",
    "        self.w = np.linalg.inv(X.T * X + self.hp_mat * np.eye(X.shape[\n",
    "            1])) * X.T * Y.T\n",
    "\n",
    "        return self.w\n",
    "\n",
    "    def get_initial_cov(self, X):\n",
    "        \"\"\"\n",
    "        get initial coviarnce matrix of w\n",
    "        \"\"\"\n",
    "\n",
    "        X = self._preprocess_X(X)\n",
    "        X = np.mat(X)\n",
    "        self.cov_w = self.hp_cov * np.eye(X.shape[1])\n",
    "\n",
    "        return self.cov_w\n",
    "\n",
    "    def get_posterior_mean(self, X, Y):\n",
    "        \"\"\"\n",
    "        get posterior mean of w\n",
    "        \"\"\"\n",
    "\n",
    "        X = self._preprocess_X(X)           #特征抽取\n",
    "        X = np.mat(X)                       #X的矩阵化\n",
    "        Y = np.mat(Y)                       #Y的矩阵化\n",
    "        cov_mat = self._get_cor_mat(X)      #获得一个叫做cov_mat的东西\n",
    "        if self.m_flag == 1:\n",
    "            self.w = self.w + self.cov_w * X.T * np.linalg.inv(\n",
    "                np.linalg.inv(cov_mat + self.hp_mat * np.eye(X.shape[0]))\n",
    "                 + X * self.cov_w * X.T + self.hp_mat * np.eye(X.shape[0]))* (Y.T - X * self.w)\n",
    "        else:\n",
    "            a1 = np.linalg.inv(cov_mat + self.hp_mat * np.eye(X.shape[0]))\n",
    "            a2 = np.linalg.inv(self.cov_w + self.hp_mat * np.eye(X.shape[\n",
    "                        1]))\n",
    "            #print(f'the shape of a1 and a2 is {a1.shape}, {a2.shape}')\n",
    "            tmp = X.T * a1 * X\n",
    "            #print(f'Xshape is {X.shape}, XT shape is {X.T.shape},tmp shape is {tmp.shape}')\n",
    "            A = np.linalg.inv(X.T * a1 * X + a2 + self.hp_mat * np.eye(X.shape[1]))\n",
    "            '''\n",
    "            A = np.linalg.inv(X.T * np.linalg.inv(\n",
    "                cov_mat + self.hp_mat * np.eye(X.shape[0])) * X\n",
    "                + np.linalg.inv(\n",
    "                    self.cov_w + self.hp_mat * np.eye(X.shape[\n",
    "                        1])) + self.hp_mat * np.eye(X.shape[1]))\n",
    "            '''\n",
    "            b1 = np.linalg.inv(cov_mat + self.hp_mat * np.eye(X.shape[0]))\n",
    "            b2 = np.linalg.inv(self.cov_w + self.hp_mat * np.eye(X.shape[1]))\n",
    "            #print(f'b1 shape is {b1.shape}, b2 shape is {b2.shape}')\n",
    "            B = (X.T * b1 * Y.T + b2 * self.w)\n",
    "            #print(f'the shape of A and B is {A.shape}, {B.shape}')\n",
    "            self.w = A * B\n",
    "\n",
    "        return self.w\n",
    "\n",
    "    def get_posterior_cov(self, X, Y):\n",
    "        \"\"\"\n",
    "        get posterior coviarnce matrix of w\n",
    "        \"\"\"\n",
    "\n",
    "        X = self._preprocess_X(X)\n",
    "        X = np.mat(X)\n",
    "        Y = np.mat(Y)\n",
    "        cov_mat = self._get_cor_mat(X)\n",
    "        self.cov_mat = np.linalg.inv(\n",
    "            np.linalg.inv(X.T * cov_mat * X + self.hp_mat * np.eye(X.shape[1]))\n",
    "            + np.linalg.inv(self.cov_w + self.hp_mat * np.eye(X.shape[\n",
    "                1])) + self.hp_mat * np.eye(X.shape[1]))\n",
    "\n",
    "        return self.cov_mat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f2889f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "475f3f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "## 0402计划\n",
    "## y的处理: 从rank变成分数\n",
    "## 方式：正太化\n",
    "class Label_Transformer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get_rankdict(self, ranks):\n",
    "        '''\n",
    "        用之前存好的数据\n",
    "        '''\n",
    "        np.random.seed(0)\n",
    "        n = len(ranks)\n",
    "        #scores = np.random.normal(0, 1, n)\n",
    "        #scores = np.random.standard_cauchy(n)\n",
    "        # scores = scores_csv['4'].values\n",
    "        a= -0.5\n",
    "        scores = skewnorm.rvs(a, size=500)\n",
    "        temp = scores.argsort()\n",
    "        ranks = np.empty_like(temp)\n",
    "        ranks[temp] = np.arange(len(scores))\n",
    "        ranks = (n - 1) - ranks\n",
    "        rankdict = {}\n",
    "        for i in range(n):\n",
    "            rankdict[ranks[i]] = scores[i]\n",
    "        return rankdict\n",
    "    \n",
    "    def rank_to_score(self, rank_dict, ranks):\n",
    "        res = np.array([rank_dict[x] for x in ranks])\n",
    "        return res\n",
    "    \n",
    "    def rank_to_score_direct(self, ranks):\n",
    "        rank_dict = self.get_rankdict(ranks)\n",
    "        score = self.rank_to_score(rank_dict, ranks)\n",
    "        return score\n",
    "        \n",
    "    \n",
    "    def score_to_rank(self, scores):\n",
    "        scores = np.array(scores)\n",
    "        temp = scores.argsort()\n",
    "        rr = np.empty_like(temp)\n",
    "        rr[temp] = np.arange(len(scores))\n",
    "        rr = rr.max() - rr\n",
    "        return rr\n",
    "            \n",
    "        \n",
    "\n",
    "#scores_csv = pd.read_csv('data/Y_score_skewness_0703.csv')\n",
    "label_transformer = Label_Transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b4ed030",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "\n",
    "\n",
    "gp_list_1 = []\n",
    "gp_list_2 = []\n",
    "knn_list = []\n",
    "\n",
    "\n",
    "for i in range(len(train_list[:])):\n",
    "    gp_list_1.append(GPNAS(2,2))\n",
    "    gp_list_2.append(GPNAS(2,2))\n",
    "    knn_list.append(KNeighborsRegressor(n_neighbors=20))\n",
    "\n",
    "\n",
    "train_num = 450\n",
    "Y_score = {}\n",
    "Y_pred_2 = []\n",
    "for i in range(8):\n",
    "    X_all_k, Y_all_k  = np.array(arch_list_train_1), np.array(train_list[i])\n",
    "    Y_all_k = label_transformer.rank_to_score_direct(Y_all_k)\n",
    "    Y_score[i] = Y_all_k\n",
    "\n",
    "\n",
    "def train_model(gp_list, index, weight_list):\n",
    "\n",
    "    for i in tqdm(range(8)):\n",
    "        if index == 1:\n",
    "            X_all_k, Y_all_k  = np.array(arch_list_train_1), np.array(train_list[i])\n",
    "        else:\n",
    "            X_all_k, Y_all_k  = np.array(arch_list_train_2), np.array(train_list[i])\n",
    "            \n",
    "        Y_all_k = Y_score[i]\n",
    "        \n",
    "        \n",
    "        weight = get_weight(**weight_list[i]['params'])\n",
    "        gp_list[i].set_task(i)\n",
    "        gp_list[i].set_weight(weight)\n",
    "        \n",
    "        \n",
    "        gp_list[i].get_initial_mean(X_all_k[0::2],Y_all_k[0::2])\n",
    "        init_cov = gp_list[i].get_initial_cov(X_all_k)\n",
    "        gp_list[i].set_task(i)\n",
    "        gp_list[i].get_posterior_mean(X_all_k,Y_all_k) \n",
    "        if index == 1:\n",
    "            knn_list[i].fit(X_all_k, Y_all_k)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4d02427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/optimweight_0703.pkl', 'rb+') as f:\n",
    "    optim_weight = pickle.load(f)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c0964eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_gp1 = optim_weight[0]\n",
    "weight_gp2 = optim_weight[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad90a148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "\n",
    "\n",
    "gp_list_1 = []\n",
    "gp_list_2 = []\n",
    "knn_list = []\n",
    "\n",
    "\n",
    "for i in range(len(train_list[:])):\n",
    "    gp_list_1.append(GPNAS(2,2))\n",
    "    gp_list_2.append(GPNAS(2,2))\n",
    "    knn_list.append(KNeighborsRegressor(n_neighbors=20))\n",
    "\n",
    "\n",
    "train_num = 450\n",
    "Y_score = {}\n",
    "Y_pred_2 = []\n",
    "for i in range(8):\n",
    "    X_all_k, Y_all_k  = np.array(arch_list_train_1), np.array(train_list[i])\n",
    "    Y_all_k = label_transformer.rank_to_score_direct(Y_all_k)\n",
    "    Y_score[i] = Y_all_k\n",
    "\n",
    "\n",
    "def train_model(gp_list, index, weight_list):\n",
    "\n",
    "    for i in tqdm(range(8)):\n",
    "        if index == 1:\n",
    "            X_all_k, Y_all_k  = np.array(arch_list_train_1), np.array(train_list[i])\n",
    "        else:\n",
    "            X_all_k, Y_all_k  = np.array(arch_list_train_2), np.array(train_list[i])\n",
    "            \n",
    "        Y_all_k = Y_score[i]\n",
    "        \n",
    "        \n",
    "        weight = get_weight(**weight_list[i]['params'])\n",
    "        gp_list[i].set_task(i)\n",
    "        gp_list[i].set_weight(weight)\n",
    "        \n",
    "        \n",
    "        gp_list[i].get_initial_mean(X_all_k[0::2],Y_all_k[0::2])\n",
    "        init_cov = gp_list[i].get_initial_cov(X_all_k)\n",
    "        gp_list[i].set_task(i)\n",
    "        gp_list[i].get_posterior_mean(X_all_k,Y_all_k) \n",
    "        if index == 1:\n",
    "            knn_list[i].fit(X_all_k, Y_all_k)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c7d87b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A,B,C shape is (101, 101), (101, 250), (250, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▎        | 1/8 [00:09<01:08,  9.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A,B,C shape is (101, 101), (101, 250), (250, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 2/8 [00:17<00:51,  8.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A,B,C shape is (101, 101), (101, 250), (250, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 3/8 [00:25<00:41,  8.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A,B,C shape is (101, 101), (101, 250), (250, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 4/8 [00:31<00:30,  7.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A,B,C shape is (101, 101), (101, 250), (250, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████▎   | 5/8 [00:38<00:21,  7.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A,B,C shape is (101, 101), (101, 250), (250, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 6/8 [00:45<00:14,  7.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A,B,C shape is (101, 101), (101, 250), (250, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 7/8 [00:52<00:06,  6.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A,B,C shape is (101, 101), (101, 250), (250, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:58<00:00,  7.37s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "train_model(gp_list_1, 1, weight_gp1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8acf9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A,B,C shape is (101, 101), (101, 250), (250, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▎        | 1/8 [00:06<00:47,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A,B,C shape is (101, 101), (101, 250), (250, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 2/8 [00:13<00:41,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A,B,C shape is (101, 101), (101, 250), (250, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 3/8 [00:20<00:34,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A,B,C shape is (101, 101), (101, 250), (250, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 4/8 [00:28<00:29,  7.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A,B,C shape is (101, 101), (101, 250), (250, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████▎   | 5/8 [00:36<00:22,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A,B,C shape is (101, 101), (101, 250), (250, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 6/8 [00:44<00:15,  7.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A,B,C shape is (101, 101), (101, 250), (250, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 7/8 [00:52<00:07,  7.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A,B,C shape is (101, 101), (101, 250), (250, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [01:05<00:00,  8.14s/it]\n"
     ]
    }
   ],
   "source": [
    "train_model(gp_list_2, 2, weight_gp2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99e7da88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ENSEMBLE模型搭建\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "import numba as nb\n",
    "import itertools\n",
    "\n",
    "\n",
    "class GPNAS_ensemble(object):\n",
    "\n",
    "    def __init__(self, c_flag, m_flag, gp1, gp2, model_knn, svr1, svr2, base_rate):\n",
    "\n",
    "        self.hp_mat = 0.0001\n",
    "        self.hp_cov = 0.01\n",
    "        self.cov_w = None\n",
    "        self.w = None\n",
    "        self.c_flag = c_flag\n",
    "        self.m_flag = m_flag\n",
    "        self.gp_1 = gp1\n",
    "        self.gp_2 = gp2\n",
    "        self.knn_model = model_knn\n",
    "        self.base_rate = base_rate\n",
    "        self.default_mode = 1\n",
    "        self.svr1 = svr1\n",
    "        self.svr2 = svr2\n",
    "    \n",
    "    def set_task(self, task):\n",
    "        self.task = task\n",
    "\n",
    "    def set_weight(self, weight):\n",
    "        self.weight = weight\n",
    "\n",
    "    \n",
    "    def _get_corelation(self, mat1, mat2):\n",
    "        return get_correlation(mat1, mat2, self.task, self.weight)\n",
    "\n",
    "    def _preprocess_X(self, X):\n",
    "\n",
    "        X = X.tolist()\n",
    "        p_X = copy.deepcopy(X)\n",
    "\n",
    "        for feature in p_X:\n",
    "            feature.append(1)\n",
    "\n",
    "        return p_X\n",
    "\n",
    "    def _get_cor_mat(self, X):\n",
    "\n",
    "        X = np.array(X)\n",
    "        l = X.shape[0]\n",
    "        cor_mat = []\n",
    "\n",
    "        for c_idx in range(l):\n",
    "            col = []\n",
    "            c_mat = X[c_idx].copy()\n",
    "\n",
    "            for r_idx in range(l):\n",
    "                r_mat = X[r_idx].copy()\n",
    "                temp_cor = self._get_corelation(c_mat, r_mat)\n",
    "                col.append(temp_cor)\n",
    "            cor_mat.append(col)\n",
    "\n",
    "        return np.mat(cor_mat)\n",
    "\n",
    "    def _get_cor_mat_joint(self, X, X_train):\n",
    "\n",
    "        res = get_cor_mat_joint(X, X_train, self.task, self.weight)\n",
    "\n",
    "        return np.mat(res)\n",
    "\n",
    "    def get_predict(self, X1, X2):\n",
    "        \"\"\"\n",
    "        get the prediction of network architecture X\n",
    "        \"\"\"\n",
    "        #X1 = self._preprocess_X(X1)\n",
    "        #X2 = self._preprocess_X(X2)\n",
    "        #X1 = np.mat(X1)\n",
    "        #X2 = np.mat(X2)\n",
    "        res1 = self.gp_1.get_predict(X1)\n",
    "        res2 = self.gp_2.get_predict(X2)\n",
    "        res3 = self.knn_model.predict(X1).reshape(-1,1)\n",
    "        res4 = self.svr1.predict(X1).reshape(-1,1)\n",
    "        res5 = self.svr1.predict(X2).reshape(-1,1)\n",
    "        #print(res1.shape, res2.shape, res3.shape)\n",
    "        res = self.base_rate[0] * res1 + self.base_rate[1] * res2 + self.base_rate[2] * res3 + self.base_rate[3] * res4 + self.base_rate[4] * res5\n",
    "        return res\n",
    "\n",
    "    def get_predict_jiont(self, X1, X2, X_train_1, X_train_2, Y_train):\n",
    "        \"\"\"\n",
    "        get the prediction of network architecture X based on X_train and Y_train\n",
    "        \"\"\"\n",
    "        #X = self._preprocess_X(X)\n",
    "        if self.default_mode == 1:\n",
    "            X1 = np.mat(X1)\n",
    "            X_train_1 = np.mat(X_train_1)\n",
    "            Y_train = np.mat(Y_train)\n",
    "            #print(X1.shape)\n",
    "            m_X = self.get_predict(X1, X2)\n",
    "            m_X_train = self.get_predict(X_train_1, X_train_2)\n",
    "            mat_train = self._get_cor_mat(X_train_1)\n",
    "            mat_joint = self._get_cor_mat_joint(X1, X_train_1)\n",
    "\n",
    "            return m_X + mat_joint * np.linalg.inv(mat_train + self.hp_mat * np.eye(\n",
    "                X_train_1.shape[0])) * (Y_train.T - m_X_train)\n",
    "        \n",
    "        elif self.default_mode == 2:\n",
    "            X2 = np.mat(X2)\n",
    "            X_train_2 = np.mat(X_train_2)\n",
    "            Y_train = np.mat(Y_train)\n",
    "            #print(X1.shape)\n",
    "            m_X = self.get_predict(X1, X2)\n",
    "            m_X_train = self.get_predict(X_train_1, X_train_2)\n",
    "            mat_train = self._get_cor_mat(X_train_2)\n",
    "            mat_joint = self._get_cor_mat_joint(X2, X_train_2)\n",
    "\n",
    "            return m_X + mat_joint * np.linalg.inv(mat_train + self.hp_mat * np.eye(\n",
    "                X_train_2.shape[0])) * (Y_train.T - m_X_train)\n",
    "        \n",
    "\n",
    "    def get_initial_mean(self, X, Y):\n",
    "        \"\"\"\n",
    "        get initial mean of w\n",
    "        \"\"\"\n",
    "\n",
    "        X = self._preprocess_X(X)\n",
    "        X = np.mat(X)\n",
    "        Y = np.mat(Y)\n",
    "        A = np.linalg.inv(X.T * X + self.hp_mat * np.eye(X.shape[\n",
    "            1]))\n",
    "        B = X.T\n",
    "        C = Y.T\n",
    "        #print(f'A,B,C shape is {A.shape}, {B.shape}, {C.shape}')\n",
    "        self.w = np.linalg.inv(X.T * X + self.hp_mat * np.eye(X.shape[\n",
    "            1])) * X.T * Y.T\n",
    "\n",
    "        return self.w\n",
    "\n",
    "    def get_initial_cov(self, X):\n",
    "        \"\"\"\n",
    "        get initial coviarnce matrix of w\n",
    "        \"\"\"\n",
    "\n",
    "        X = self._preprocess_X(X)\n",
    "        X = np.mat(X)\n",
    "        self.cov_w = self.hp_cov * np.eye(X.shape[1])\n",
    "\n",
    "        return self.cov_w\n",
    "\n",
    "    def get_posterior_mean(self, X, Y):\n",
    "        \"\"\"\n",
    "        get posterior mean of w\n",
    "        \"\"\"\n",
    "\n",
    "        X = self._preprocess_X(X)           #特征抽取\n",
    "        X = np.mat(X)                       #X的矩阵化\n",
    "        Y = np.mat(Y)                       #Y的矩阵化\n",
    "        cov_mat = self._get_cor_mat(X)      #获得一个叫做cov_mat的东西\n",
    "        if self.m_flag == 1:\n",
    "            self.w = self.w + self.cov_w * X.T * np.linalg.inv(\n",
    "                np.linalg.inv(cov_mat + self.hp_mat * np.eye(X.shape[0]))\n",
    "                 + X * self.cov_w * X.T + self.hp_mat * np.eye(X.shape[0]))* (Y.T - X * self.w)\n",
    "        else:\n",
    "            a1 = np.linalg.inv(cov_mat + self.hp_mat * np.eye(X.shape[0]))\n",
    "            a2 = np.linalg.inv(self.cov_w + self.hp_mat * np.eye(X.shape[\n",
    "                        1]))\n",
    "            #print(f'the shape of a1 and a2 is {a1.shape}, {a2.shape}')\n",
    "            tmp = X.T * a1 * X\n",
    "            #print(f'Xshape is {X.shape}, XT shape is {X.T.shape},tmp shape is {tmp.shape}')\n",
    "            A = np.linalg.inv(X.T * a1 * X + a2 + self.hp_mat * np.eye(X.shape[1]))\n",
    "            b1 = np.linalg.inv(cov_mat + self.hp_mat * np.eye(X.shape[0]))\n",
    "            b2 = np.linalg.inv(self.cov_w + self.hp_mat * np.eye(X.shape[1]))\n",
    "            #print(f'b1 shape is {b1.shape}, b2 shape is {b2.shape}')\n",
    "            B = (X.T * b1 * Y.T + b2 * self.w)\n",
    "            #print(f'the shape of A and B is {A.shape}, {B.shape}')\n",
    "            self.w = A * B\n",
    "\n",
    "        return self.w\n",
    "\n",
    "    def get_posterior_cov(self, X, Y):\n",
    "        \"\"\"\n",
    "        get posterior coviarnce matrix of w\n",
    "        \"\"\"\n",
    "\n",
    "        X = self._preprocess_X(X)\n",
    "        X = np.mat(X)\n",
    "        Y = np.mat(Y)\n",
    "        cov_mat = self._get_cor_mat(X)\n",
    "        self.cov_mat = np.linalg.inv(\n",
    "            np.linalg.inv(X.T * cov_mat * X + self.hp_mat * np.eye(X.shape[1]))\n",
    "            + np.linalg.inv(self.cov_w + self.hp_mat * np.eye(X.shape[\n",
    "                1])) + self.hp_mat * np.eye(X.shape[1]))\n",
    "\n",
    "        return self.cov_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57e86353",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [00:13<01:34, 13.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kendalltau: KendalltauResult(correlation=0.292929292929293, pvalue=1.572574511381732e-05)\n",
      "mae: 0.7138676611127234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 2/8 [00:26<01:20, 13.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kendalltau: KendalltauResult(correlation=0.8844444444444446, pvalue=7.4192456715054e-39)\n",
      "mae: 0.15426513999897484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 3/8 [00:39<01:06, 13.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kendalltau: KendalltauResult(correlation=0.9006060606060607, pvalue=3.170961464878882e-40)\n",
      "mae: 0.11448184120791685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 4/8 [00:53<00:53, 13.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kendalltau: KendalltauResult(correlation=0.9547474747474749, pvalue=5.442733109497578e-45)\n",
      "mae: 0.057071903876550235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████▎   | 5/8 [01:06<00:39, 13.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kendalltau: KendalltauResult(correlation=0.9034343434343436, pvalue=1.8157914410327123e-40)\n",
      "mae: 0.10459312666241322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 6/8 [01:19<00:26, 13.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kendalltau: KendalltauResult(correlation=0.6876767676767678, pvalue=3.7657767599235074e-24)\n",
      "mae: 0.3275186511477446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 7/8 [01:34<00:13, 13.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kendalltau: KendalltauResult(correlation=0.9349494949494952, pvalue=3.2381683081576764e-43)\n",
      "mae: 0.09341963099699266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [01:47<00:00, 13.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kendalltau: KendalltauResult(correlation=0.7866666666666667, pvalue=4.278422751584664e-31)\n",
      "mae: 0.24298248893407487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## 训练Ensemble模型\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.svm import SVR\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "gp_ensembles = []\n",
    "'''\n",
    "gp_list_1 = []\n",
    "gp_list_2 = []\n",
    "knn_list = []\n",
    "'''\n",
    "for i in range(8):\n",
    "    gp1 = gp_list_1[i]\n",
    "    gp2 = gp_list_2[i]\n",
    "    \n",
    "    weight = get_weight(**weight_gp1[i]['params'])\n",
    "    gp1.set_weight(weight)\n",
    "    \n",
    "    weight = get_weight(**weight_gp2[i]['params'])\n",
    "    gp2.set_weight(weight)\n",
    "    \n",
    "    '''\n",
    "    X_all_k_1, X_all_k_2  = np.array(arch_list_train_1), np.array(arch_list_train_2)\n",
    "    Y_all_k = Y_score[i]\n",
    "    \n",
    "    gp_list_1[i].set_task(i)\n",
    "    gp_list_2[i].set_task(i)\n",
    "    \n",
    "    \n",
    "    gp_list_1[i].get_initial_mean(X_all_k[0::2],Y_all_k[0::2])\n",
    "    gp_list_2[i].get_initial_mean(X_all_k[0::2],Y_all_k[0::2])\n",
    "    \n",
    "    init_cov = gp_list_1[i].get_initial_cov(X_all_k)\n",
    "    init_cov = gp_list_2[i].get_initial_cov(X_all_k)\n",
    "    \n",
    "    gp_list_1[i].get_posterior_mean(X_all_k[1::2],Y_all_k[1::2]) \n",
    "    gp_list_2[i].get_posterior_mean(X_all_k[1::2],Y_all_k[1::2]) \n",
    "    '''\n",
    "\n",
    "    model_knn = knn_list[i]\n",
    "    if i == 0:\n",
    "        base_rate = [0.3, 0.4, 0.3, 0, 0]\n",
    "    elif i == 5:\n",
    "        base_rate = [0.4, 0.6, 0, 0, 0]\n",
    "    else:\n",
    "        base_rate = [0.35, 0.55, 0, 0.05, 0.05] \n",
    "        \n",
    "    svr1 = SVR(kernel=\"rbf\", gamma=0.012)\n",
    "    svr2 = SVR(kernel=\"rbf\", gamma=0.018)\n",
    "        \n",
    "    gp_ensembles.append(GPNAS_ensemble(2, 2, gp1, gp2, model_knn, svr1, svr2, base_rate))\n",
    "    \n",
    "    weight = get_weight(**weight_gp1[i]['params'])\n",
    "    gp_ensembles[i].set_weight(weight)\n",
    "    \n",
    "    \n",
    "for i in tqdm(range(8)):\n",
    "    #默认是1\n",
    "    X_all_k_1, X_all_k_2  = np.array(arch_list_train_1), np.array(arch_list_train_2)\n",
    "    Y_all_k = Y_score[i]\n",
    "    X_train_k_1, X_test_k_1, Y_train_k_1, Y_test_k_1 = train_test_split(X_all_k_1, Y_all_k, test_size=0.2, random_state=4)\n",
    "    X_train_k_2, X_test_k_2, Y_train_k_2, Y_test_k_2 = train_test_split(X_all_k_2, Y_all_k, test_size=0.2, random_state=4)\n",
    "    \n",
    "    X_all_k_1, Y_all_k  = np.array(arch_list_train_1), np.array(train_list[i])\n",
    "    Y_all_k = Y_score[i]\n",
    "    X_train_k, X_test_k, Y_train_k, Y_test_k = train_test_split(X_all_k, Y_all_k, test_size=0.2, random_state=4)\n",
    "    \n",
    "    gp_ensembles[i].svr1.fit(X_all_k_1, Y_all_k)\n",
    "    gp_ensembles[i].svr2.fit(X_all_k_2, Y_all_k)\n",
    "    \n",
    "    gp_ensembles[i].get_initial_mean(X_train_k_1[0::2],Y_train_k[0::2])\n",
    "    init_cov = gp_ensembles[i].get_initial_cov(X_train_k_1)\n",
    "    gp_ensembles[i].set_task(i)\n",
    "    gp_ensembles[i].get_posterior_mean(X_train_k_1[1::2],Y_train_k[1::2]) \n",
    "    \n",
    "    \n",
    "    y_predict = gp_ensembles[i].get_predict_jiont(X_test_k_1, X_test_k_2, X_train_k_1[::1], X_train_k_2[::1],\n",
    "                                             Y_train_k[::1])\n",
    "                                        \n",
    "\n",
    "    print('Kendalltau:',scipy.stats.stats.kendalltau( y_predict,Y_test_k))\n",
    "    print('mae:', mean_absolute_error(y_predict,Y_test_k))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee7cf804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cplfw_rank': 0,\n",
       " 'market1501_rank': 0,\n",
       " 'dukemtmc_rank': 0,\n",
       " 'msmt17_rank': 0,\n",
       " 'veri_rank': 0,\n",
       " 'vehicleid_rank': 0,\n",
       " 'veriwild_rank': 0,\n",
       " 'sop_rank': 0,\n",
       " 'arch': 'j121221121221221311331321121221000000'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#试一下结果吧\n",
    "with open('CVPR_2022_NAS_Track2_test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "test_data['arch99997']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d72c4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0]\n",
      "[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "test_arch_list_1 = []\n",
    "for key in test_data.keys():\n",
    "    test_arch =  processor_1.convert_x(test_data[key]['arch'])\n",
    "    test_arch_list_1.append(test_arch)\n",
    "print(test_arch_list_1[99499])\n",
    "\n",
    "test_arch_list_2 = []\n",
    "for key in test_data.keys():\n",
    "    test_arch =  processor_2.convert_x(test_data[key]['arch'])\n",
    "    test_arch_list_2.append(test_arch)\n",
    "print(test_arch_list_2[99499])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48cb8d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict the rank of: 1\n"
     ]
    }
   ],
   "source": [
    "name_list = ['market1501_rank']\n",
    "my_reses = []\n",
    "\n",
    "rank_all = []\n",
    "for task in [1]:\n",
    "    print('Predict the rank of:', task)\n",
    "    Y = Y_score[task]\n",
    "    my_res = gp_ensembles[task].get_predict_jiont(np.array(test_arch_list_1),\n",
    "                                                 np.array(test_arch_list_2),\n",
    "                                                 np.array(arch_list_train_1),\n",
    "                                                 np.array(arch_list_train_2), Y)\n",
    "    my_reses.append(my_res)\n",
    "    rank_all.append(np.mat(label_transformer.score_to_rank(np.array(my_res.ravel())[0]).reshape(-1,1)))\n",
    "    # fast mode\n",
    "    #rank_all.append(gp_list[task].get_predict(np.array(test_arch_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cc5b1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to save results!\n"
     ]
    }
   ],
   "source": [
    "for idx,key in enumerate(test_data.keys()):\n",
    "    #test_data[key]['cplfw_rank'] = int(rank_all_2[0][idx][0])\n",
    "    test_data[key]['market1501_rank'] = int(rank_all[0][idx][0])\n",
    "    #test_data[key]['dukemtmc_rank'] = int(rank_all[2][idx][0])\n",
    "    #test_data[key]['msmt17_rank'] = int(rank_all[3][idx][0])\n",
    "    #test_data[key]['veri_rank'] = int(rank_all[4][idx][0])\n",
    "    #test_data[key]['vehicleid_rank'] = int(rank_all[5][idx][0])\n",
    "    #test_data[key]['veriwild_rank'] = int(rank_all[6][idx][0])\n",
    "    #test_data[key]['sop_rank'] = int(rank_all[7][idx][0])\n",
    "print('Ready to save results!')\n",
    "with open('./CVPR_2022_NAS_Track2_submit_task1_0510version.json', 'w') as f:\n",
    "    json.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394ddbaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3293ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
